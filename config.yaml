feature_dir: ""  
tokenizer_path: "processed_data/english_tokenizer.json"
model_save_path: "cslr_translator_model.pth"
input_dim: 1123  # 99 (MediaPipe) + 1024 (I3D)
output_dim: 5000  # Vocab size
hid_dim: 512
n_layers: 6
n_heads: 8
pf_dim: 2048
dropout: 0.1
learning_rate: 0.0001
batch_size: 2
num_epochs: 1  
patience: 10
augment_prob: 0.5
max_seq_len: 100
tsv_dir: ""  
vocab_size: 5000
